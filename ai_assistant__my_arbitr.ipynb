{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pandser/ai_assistant/blob/main/ai_assistant__my_arbitr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Практическая работа.**\n",
        "\n",
        "**Создание неро-сотрудника на основе больших языковых моделей.**\n",
        "\n",
        "В данной практической работе представлен ИИ-ассистент, косультирующий пользователя по вопросам подачи документов в Арбитражные суды РФ через систему \"Мой арбитр\"."
      ],
      "metadata": {
        "id": "scM_uA5ykSMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Установка необходимых библиотек."
      ],
      "metadata": {
        "id": "vaVvevoQyJWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "transformers>=4.42.0\n",
        "llama_index\n",
        "pyvis==0.3.2\n",
        "Ipython==7.34.0\n",
        "langchain==0.2.5\n",
        "pypdf==4.2.0\n",
        "langchain_community==0.2.5\n",
        "llama-index-llms-huggingface==0.2.3\n",
        "llama-index-embeddings-huggingface==0.2.2\n",
        "llama-index-embeddings-langchain==0.1.2\n",
        "langchain-huggingface==0.0.3\n",
        "sentencepiece==0.1.99\n",
        "accelerate==0.31.0\n",
        "bitsandbytes\n",
        "peft==0.11.1\n",
        "llama-index-readers-wikipedia==0.1.4\n",
        "wikipedia==1.4.0\n",
        "llama-index-readers-file\n",
        "gradio\n",
        "\n",
        "arize-phoenix\n",
        "gcsfs\n",
        "nest-asyncio\n",
        "openinference-instrumentation-llama-index\n",
        "opentelemetry-api\n",
        "opentelemetry-sdk\n",
        "opentelemetry-exporter-otlp\n",
        "\n",
        "# Зависимости\n",
        "huggingface-hub==0.23.3\n",
        "torch>=2.3.1\n",
        "numpy==1.25.2\n",
        "packaging==24.1\n",
        "pyyaml==6.0.1\n",
        "requests==2.31.0\n",
        "tqdm==4.66.4\n",
        "filelock==3.14.0\n",
        "regex==2024.5.15\n",
        "typing-extensions==4.12.2\n",
        "safetensors==0.4.3\n",
        "tokenizers==0.19.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ0Logd_Au-z",
        "outputId": "77e85993-b02a-4c18-8e83-10213571f322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "QkjnXY7IAy3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подготовка данных\n"
      ],
      "metadata": {
        "id": "NcZ-X3WjNyS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим базу знаний. Источником будет справочный раздел сайта [\"Мой арбитр\"](https://my.arbitr.ru/)"
      ],
      "metadata": {
        "id": "J2Va8mFbyoZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data # создание директории в которой будут собраны данные"
      ],
      "metadata": {
        "id": "oSTM6QQQXDxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "url = (\"https://my.arbitr.ru/#help/4/0/\")\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')"
      ],
      "metadata": {
        "id": "FQ8WR0QlWr4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faq = soup.find_all('script', id=re.compile('^help*')) # поиск всех тегов <script>, id которых начинается на 'help'"
      ],
      "metadata": {
        "id": "loDnRhm2W8BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num, data in enumerate(faq):\n",
        "    raw = data.string.strip()\n",
        "    if raw.startswith('<p') or raw.startswith('<h2'): # фильтруем теги в которых содержится интересующий нас текст\n",
        "        text = re.sub(re.compile('<.*?>'), '', raw) # очищаем текст от тегов\n",
        "        with open(file=f'data/{num}.txt', mode='w', encoding='utf-8') as f:\n",
        "            f.writelines(text) # запись полученного текста в файл"
      ],
      "metadata": {
        "id": "GRTD6e1BXJ6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подготовка модели"
      ],
      "metadata": {
        "id": "zsR2JgvtXeEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from huggingface_hub import login\n",
        "from langchain_huggingface  import HuggingFaceEmbeddings\n",
        "from llama_index.core import (\n",
        "    Settings,\n",
        "    SimpleDirectoryReader,\n",
        ")\n",
        "from llama_index.core.graph_stores import SimpleGraphStore\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.readers.file import PDFReader\n",
        "from peft import PeftModel, PeftConfig\n",
        "from pyvis.network import Network\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    GenerationConfig,\n",
        ")"
      ],
      "metadata": {
        "id": "kR501zDpAztj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['HUGGINGFACE_ACCESS_TOKEN'] = getpass.getpass('Введите API Key:')\n",
        "login(os.environ['HUGGINGFACE_ACCESS_TOKEN'])"
      ],
      "metadata": {
        "id": "FUWC6OsXA39Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def messages_to_prompt(messages):\n",
        "    prompt = \"\"\n",
        "    for message in messages:\n",
        "        if message.role == 'system':\n",
        "            prompt += f\"<s>{message.role}\\n{message.content}</s>\\n\"\n",
        "        elif message.role == 'user':\n",
        "            prompt += f\"<s>{message.role}\\n{message.content}</s>\\n\"\n",
        "        elif message.role == 'bot':\n",
        "            prompt += f\"<s>bot\\n\"\n",
        "\n",
        "    # ensure we start with a system prompt, insert blank if needed\n",
        "    if not prompt.startswith(\"<s>system\\n\"):\n",
        "        prompt = \"<s>system\\n</s>\\n\" + prompt\n",
        "\n",
        "    # add final assistant prompt\n",
        "    prompt = prompt + \"<s>bot\\n\"\n",
        "    return prompt\n",
        "\n",
        "def completion_to_prompt(completion):\n",
        "    return f\"<s>system\\n</s>\\n<s>user\\n{completion}</s>\\n<s>bot\\n\""
      ],
      "metadata": {
        "id": "zLUrfdZiBF8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Задаем имя модели\n",
        "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
        "\n",
        "# Создание конфига, соответствующего методу PEFT (в нашем случае LoRA)\n",
        "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Загружаем базовую модель, ее имя берем из конфига для LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,          # идентификатор модели\n",
        "    quantization_config=quantization_config, # параметры квантования\n",
        "    torch_dtype=torch.float16,               # тип данных\n",
        "    device_map=\"auto\"                        # автоматический выбор типа устройства\n",
        ")\n",
        "\n",
        "# Загружаем LoRA модель\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Переводим модель в режим инференса\n",
        "# Можно не переводить, но явное всегда лучше неявного\n",
        "model.eval()\n",
        "\n",
        "# Загружаем токенизатор\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)"
      ],
      "metadata": {
        "id": "iAFq3MzeBJXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "print(generation_config)"
      ],
      "metadata": {
        "id": "FeTNOw6PCVk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceLLM(\n",
        "    model=model,             # модель\n",
        "    model_name=MODEL_NAME,   # идентификатор модели\n",
        "    tokenizer=tokenizer,     # токенизатор\n",
        "    max_new_tokens=generation_config.max_new_tokens, # параметр необходимо использовать здесь, и не использовать в generate_kwargs, иначе ошибка двойного использования\n",
        "    model_kwargs={\"quantization_config\": quantization_config}, # параметры квантования\n",
        "    generate_kwargs = {   # параметры для инференса\n",
        "      \"bos_token_id\": generation_config.bos_token_id, # токен начала последовательности\n",
        "      \"eos_token_id\": generation_config.eos_token_id, # токен окончания последовательности\n",
        "      \"pad_token_id\": generation_config.pad_token_id, # токен пакетной обработки (указывает, что последовательность ещё не завершена)\n",
        "      \"no_repeat_ngram_size\": generation_config.no_repeat_ngram_size,\n",
        "      \"repetition_penalty\": generation_config.repetition_penalty,\n",
        "      \"temperature\": generation_config.temperature,\n",
        "      \"do_sample\": True,\n",
        "      \"top_k\": 50,\n",
        "      \"top_p\": 0.95\n",
        "    },\n",
        "    messages_to_prompt=messages_to_prompt,     # функция для преобразования сообщений к внутреннему формату\n",
        "    completion_to_prompt=completion_to_prompt, # функции для генерации текста\n",
        "    device_map=\"auto\",                         # автоматически определять устройство\n",
        ")"
      ],
      "metadata": {
        "id": "GMTvsdCeCcJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(\n",
        "      model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "ByICOPNCCgmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка ServiceContext (глобальная настройка параметров LLM)\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512"
      ],
      "metadata": {
        "id": "xF3mjkOUCjoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Подключение поисковика Auto-Merging Retriever из LlamaHub"
      ],
      "metadata": {
        "id": "5sh5C1NHyw37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llama_pack import download_llama_pack\n",
        "\n",
        "AutoMergingRetrieverPack = download_llama_pack(\n",
        "    \"AutoMergingRetrieverPack\",\n",
        "    \"./auto_merging_retriever_pack\",\n",
        ")"
      ],
      "metadata": {
        "id": "aI5eHQDbGT7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = SimpleDirectoryReader(\n",
        "    \"./data\",\n",
        ").load_data()"
      ],
      "metadata": {
        "id": "FiUL-HjDYi1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_merging_pack = AutoMergingRetrieverPack(docs)"
      ],
      "metadata": {
        "id": "kg4Po-UhYb9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подключение Phoenix\n",
        "Наблюдени за приложением LlamaIndex."
      ],
      "metadata": {
        "id": "wnYYmU0yxdTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import phoenix as px\n",
        "\n",
        "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
        "from phoenix.trace import DocumentEvaluations, SpanEvaluations"
      ],
      "metadata": {
        "id": "W7oJPo4hzryi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nest_asyncio.apply()  # необходим для параллельных вычислений в среде ноутбуков"
      ],
      "metadata": {
        "id": "cgRNFIdVz1O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = px.launch_app()"
      ],
      "metadata": {
        "id": "Dwb86VOaz7bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Не удалось подключиться к Phoenix из-за ошибки 403 Forbidden."
      ],
      "metadata": {
        "id": "KOgnM81syWXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Запросы к ассистенту"
      ],
      "metadata": {
        "id": "Hv1yDrsN1UZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(query):\n",
        "    message_template =f\"\"\"<s>system\n",
        "        Ты ассистент для сайта 'Мой арбитр' https://my.arbitr.ru/.\n",
        "        Отвечай в соответствии с Источником.\n",
        "        Проверь, есть ли в Источнике упоминания о ключевых словах Вопроса.\n",
        "        Если нет, то просто скажи: 'я не знаю'. Не придумывай! </s>\n",
        "        <s>user\n",
        "        Вопрос: {query}\n",
        "        Источник:\n",
        "        </s>\n",
        "        \"\"\"\n",
        "    return str(auto_merging_pack.run(message_template))"
      ],
      "metadata": {
        "id": "n8WpXBSa18z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'Как подать документы от имени юридического лица?'\n",
        "print(get_response(query))"
      ],
      "metadata": {
        "id": "8LKP33GGvgqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d098f48-a164-4fc6-c8b1-0ec161f3e4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Важно учесть, что подача документов от имени юридического лица может быть выполнена только представителем такого лица. В зависимости от формата подачи документов, представление может быть осуществлено на бумажной основе или онлайн через интернет.\n",
            "\n",
            "Если документы подаются на бумажной основе, они должны быть подписаны представителем юридического лица и удостоверенные печатью. Кроме того, следует предоставить копию паспорта представителя и свидетельство о его полномочиях.\n",
            "\n",
            "Если документы подаются онлайн, то представитель должен залогиниться в систему и выполнить процедуру подачи документов. Для этого необходимо заполнить все необходимые поля и предоставить необходимые документы в электронном виде.\n",
            "\n",
            "После подачи документов, их можно проверить в режиме онлайн.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'Можно ли отправить документ без подписания электронной подписью?'\n",
        "print(get_response(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W0jPzxFcc83",
        "outputId": "f30b77ce-a64a-4e69-ca99-59dc57bac9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Merging 2 nodes into parent node.\n",
            "> Parent node id: 8c025f6d-8b1a-4db6-b32d-0813a2e2b39a.\n",
            "> Parent node text: Порядок подачи документов не предусматривает подачу электронных документов, подписанных присоедин...\n",
            "\n",
            "Да, можно. В зависимости от того, на что вы хотите отправить документ, может быть разные требования. Например, для отправки документа в государственные органы, обычно требуется подписание электронной подписью. Но для отправки документа в частные организации или людей, это может быть необязательным.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вывод\n",
        "Создан ИИ-ассистент на основе модели saiga_mistral_7b.\n",
        "\n",
        "Подготовлена база знаний из открытого источника.\n",
        "\n",
        "Для поиска документов использован пакет Auto-Merging Retriever из LlamaHub.\n",
        "\n",
        "Не удалось подключиться к Phoenix из-за ошибки 403 Forbidden.\n",
        "Для обеспечения безопасности планировал использовать meta-llama/LlamaGuard-7b, но запрос на использование модели был отклонен.\n"
      ],
      "metadata": {
        "id": "lgO0bHKWrANH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}